# Implementation Plan
**Generated**: 2026-01-01 00:35 UTC
**Generated By**: Scout-and-Plan Agent (combined workflow)
**Task ID**: GitHub Issue #21 - Technical SEO: Validate and enhance robots.txt
**Estimated Build Time**: 0.5 hours
**Complexity**: Low

## Investigation Summary

### Request Analysis
**Type**: SEO / Technical
**Source**: GitHub Issue #21
**Priority**: Medium

### Task Classification
**Category**: CHORE (validation/enhancement of existing file)
**Test Strategy**: QUICK (build + lint only - static file update)

### Current State Assessment

**File Location**: `/home/pbrown/aireadypdx/public/robots.txt` (616 bytes, 31 lines)

**Current robots.txt Content**:
```
# robots.txt for aireadypdx.com

User-agent: *
Allow: /
Disallow: /novus-migration-status

# Block all AI/LLM crawlers from Novus page
User-agent: GPTBot
Disallow: /novus-migration-status

User-agent: ChatGPT-User
Disallow: /novus-migration-status

User-agent: Google-Extended
Disallow: /novus-migration-status

User-agent: CCBot
Disallow: /novus-migration-status

User-agent: anthropic-ai
Disallow: /novus-migration-status

User-agent: Claude-Web
Disallow: /novus-migration-status

# Sitemap location
Sitemap: https://aireadypdx.com/sitemap.xml

# LLMs.txt for AI crawlers
LLMs-txt: https://aireadypdx.com/llms.txt
```

**Already Implemented** (from previous issues):
- [x] Sitemap reference (Issue #3) - Line 27
- [x] LLMs-txt reference (Issue #5) - Line 30
- [x] AI crawler rules for Novus page blocking

**Issue #21 Recommended Enhancements Analysis**:

| Enhancement | Status | Notes |
|-------------|--------|-------|
| Add Sitemap reference | DONE | Already at line 27 |
| Review AI crawler policies | NEEDS UPDATE | See analysis below |
| Consider Crawl-delay | NOT RECOMMENDED | See analysis below |

### AI Crawler Analysis

**Current AI Crawlers Listed**:
1. GPTBot (OpenAI) - Current
2. ChatGPT-User (OpenAI) - Current
3. Google-Extended (Google) - Current
4. CCBot (Common Crawl) - Current
5. anthropic-ai (Anthropic) - **DEPRECATED**
6. Claude-Web (Anthropic) - **DEPRECATED**

**Missing AI Crawlers (2025 standard)**:
1. **ClaudeBot** - Anthropic's current active crawler (replaced anthropic-ai and Claude-Web in July 2024)
2. **PerplexityBot** - Perplexity.ai's crawler (significant traffic growth in 2025)
3. **OAI-SearchBot** - OpenAI's search-focused crawler

**Current Issue**: The robots.txt references deprecated Anthropic crawlers (`anthropic-ai`, `Claude-Web`) but is missing the current `ClaudeBot` user agent that replaced them.

### Crawl-delay Analysis

**Research Findings**:
- Google completely ignores Crawl-delay directive
- Bing respects it but recommends using Webmaster Tools instead
- Only useful for Yandex and older crawlers
- Modern crawlers can self-throttle when detecting server load
- For a small marketing site on modern hosting, crawl-delay is unnecessary

**Recommendation**: Do NOT add Crawl-delay. It adds no value for this use case and is not recognized by major search engines.

### Complexity Assessment
**Complexity**: Simple
**Effort**: 0.5 hours
**Risk**: Low (static file update, no code changes)

### Patterns Identified
**Primary**: Existing robots.txt structure with User-agent blocks
**Reference**: `/home/pbrown/aireadypdx/public/robots.txt` lines 8-24

---

## Executive Summary

The robots.txt file is already well-configured with sitemap and llms.txt references from previous issues. The primary enhancement needed is updating AI crawler user agents to reflect 2025 standards: replacing deprecated Anthropic crawlers (anthropic-ai, Claude-Web) with the current ClaudeBot, and adding PerplexityBot and OAI-SearchBot. No Crawl-delay is needed for this site.

---

## Phase 1: Update AI Crawler User Agents

### Subtask 1.1: Replace deprecated Anthropic crawlers with ClaudeBot
**File**: `/home/pbrown/aireadypdx/public/robots.txt`
**Pattern**: Follow existing User-agent block structure
**Instructions**:
1. Remove the `anthropic-ai` block (lines 20-21)
2. Replace the `Claude-Web` block (lines 23-24) with `ClaudeBot`
3. The ClaudeBot entry should be:
```
User-agent: ClaudeBot
Disallow: /novus-migration-status
```

**Validation**:
```bash
grep -n "ClaudeBot\|anthropic-ai\|Claude-Web" /home/pbrown/aireadypdx/public/robots.txt
```

**Completion Criteria**:
- [ ] `anthropic-ai` block removed
- [ ] `Claude-Web` block removed
- [ ] `ClaudeBot` block added with correct Disallow rule

### Subtask 1.2: Add PerplexityBot user agent
**File**: `/home/pbrown/aireadypdx/public/robots.txt`
**Pattern**: Follow existing User-agent block structure
**Instructions**:
1. Add PerplexityBot block after CCBot block:
```
User-agent: PerplexityBot
Disallow: /novus-migration-status
```

**Validation**:
```bash
grep -n "PerplexityBot" /home/pbrown/aireadypdx/public/robots.txt
```

**Completion Criteria**:
- [ ] PerplexityBot block added with correct Disallow rule

### Subtask 1.3: Add OAI-SearchBot user agent
**File**: `/home/pbrown/aireadypdx/public/robots.txt`
**Pattern**: Follow existing User-agent block structure
**Instructions**:
1. Add OAI-SearchBot block after ChatGPT-User block:
```
User-agent: OAI-SearchBot
Disallow: /novus-migration-status
```

**Validation**:
```bash
grep -n "OAI-SearchBot" /home/pbrown/aireadypdx/public/robots.txt
```

**Completion Criteria**:
- [ ] OAI-SearchBot block added with correct Disallow rule

---

## Phase 2: Organize and Clean Up

### Subtask 2.1: Reorganize AI crawler section with updated comments
**File**: `/home/pbrown/aireadypdx/public/robots.txt`
**Instructions**:
1. Update the comment to reflect the current state
2. Group AI crawlers logically (OpenAI, Anthropic, Google, Common Crawl, Perplexity)

**Final robots.txt should be**:
```
# robots.txt for aireadypdx.com

User-agent: *
Allow: /
Disallow: /novus-migration-status

# AI/LLM crawlers - block Novus page only, allow main content
# OpenAI
User-agent: GPTBot
Disallow: /novus-migration-status

User-agent: ChatGPT-User
Disallow: /novus-migration-status

User-agent: OAI-SearchBot
Disallow: /novus-migration-status

# Anthropic
User-agent: ClaudeBot
Disallow: /novus-migration-status

# Google AI
User-agent: Google-Extended
Disallow: /novus-migration-status

# Common Crawl
User-agent: CCBot
Disallow: /novus-migration-status

# Perplexity
User-agent: PerplexityBot
Disallow: /novus-migration-status

# Sitemap location
Sitemap: https://aireadypdx.com/sitemap.xml

# LLMs.txt for AI crawlers
LLMs-txt: https://aireadypdx.com/llms.txt
```

**Validation**:
```bash
npm run build --prefix /home/pbrown/aireadypdx
cat /home/pbrown/aireadypdx/dist/robots.txt
```

**Completion Criteria**:
- [ ] robots.txt has organized AI crawler sections
- [ ] All deprecated crawlers removed (anthropic-ai, Claude-Web)
- [ ] All current crawlers present (GPTBot, ChatGPT-User, OAI-SearchBot, ClaudeBot, Google-Extended, CCBot, PerplexityBot)
- [ ] Sitemap reference preserved
- [ ] LLMs-txt reference preserved
- [ ] Build passes

---

## Phase 3: Validation

### Subtask 3.1: Verify build and file deployment
**Instructions**:
1. Run production build
2. Verify robots.txt in dist folder
3. Run lint check

**Validation**:
```bash
npm run build --prefix /home/pbrown/aireadypdx
npm run lint --prefix /home/pbrown/aireadypdx
cat /home/pbrown/aireadypdx/dist/robots.txt
```

**Completion Criteria**:
- [ ] Build passes without errors
- [ ] Lint passes without errors
- [ ] dist/robots.txt matches public/robots.txt

---

## Summary of Deliverables

**Files Modified**:
- `/home/pbrown/aireadypdx/public/robots.txt`

**Changes Made**:
1. Removed deprecated Anthropic crawlers (anthropic-ai, Claude-Web)
2. Added ClaudeBot (current Anthropic crawler)
3. Added PerplexityBot (Perplexity.ai crawler)
4. Added OAI-SearchBot (OpenAI search crawler)
5. Reorganized with clearer comments grouping by company
6. Preserved existing Sitemap and LLMs-txt references

**Not Implemented** (with rationale):
- Crawl-delay: Not recommended for modern sites; ignored by Google, unnecessary for small marketing site

---

## Handoff to Build Agent

1. Read current robots.txt file first
2. Apply the complete replacement in Subtask 2.1 (simpler than incremental edits)
3. Run `npm run build` to verify
4. Run `npm run lint` to verify
5. Verify dist/robots.txt content matches expectations

---

## Performance Metrics

| Phase | Duration |
|-------|----------|
| Issue Classification | 1m |
| Investigation | 5m |
| Pattern Identification | 2m |
| Validation | 3m |
| Planning | 4m |
| **Total** | **15m** |

---

## Insights & Process Improvements (REQUIRED)

### Issues Encountered During Investigation
1. **Deprecated crawler names** - The current robots.txt uses `anthropic-ai` and `Claude-Web` which were deprecated in July 2024. Resolution: Web search confirmed ClaudeBot is the current Anthropic crawler.
2. **Crawl-delay uncertainty** - Issue suggested considering Crawl-delay but didn't provide clear guidance. Resolution: Web search confirmed it's ignored by Google and not needed for small sites.

### Input Quality Assessment
**Clarity of Original Request**: 4/5
**Missing Information**:
- No guidance on which specific AI crawlers are most important in 2025
- Issue didn't note that Sitemap reference was already added by Issue #3
**Suggested Input Improvements**:
- Cross-reference related issues that may have already implemented some recommendations
- Include links to authoritative sources for AI crawler user agent lists

### Process Improvement Suggestions

**For Future Scout-and-Plan Runs**:
1. Always check completion-docs for related issues before investigating - Issue #3 and #5 had already implemented the sitemap and llms.txt references
2. For SEO/technical tasks, web search for current best practices is essential as the landscape changes rapidly

**For Build Agent**:
1. The complete file replacement approach (Subtask 2.1) is simpler than incremental edits for this task
2. The final robots.txt content is provided exactly - use it verbatim

**For Overall Workflow**:
1. Consider adding a "pre-investigation" step that checks completion-docs for related issues
2. SEO issues should include a "last verified date" since crawler landscapes change frequently

### Decision Log
| Decision | Rationale |
|----------|-----------|
| No Crawl-delay | Google ignores it; modern crawlers self-throttle; small site doesn't need it |
| Keep Disallow-only approach | Matches issue intent: block Novus page, allow main content for AI/AEO |
| Add company grouping comments | Improves maintainability and clarity |
| Remove deprecated crawlers | anthropic-ai and Claude-Web were deprecated July 2024 |
